{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib as matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Import basic packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv('creditcard.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train['Amount_n']= StandardScaler().fit_transform(train['Amount'].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>Amount_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9    ...          V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787    ...     0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425    ...    -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654    ...     0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024    ...     0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739    ...     0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  Amount_n  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  0.244964  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0 -0.342475  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  1.160686  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  0.140534  \n",
       "4  0.502292  0.219422  0.215153   69.99      0 -0.073403  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>Amount_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10    ...          V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794    ...    -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974    ...    -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643    ...     0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952    ...    -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074    ...    -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  Amount_n  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0 -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0 -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= train.drop(['Time','Amount'], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "X= train.ix[:, train.columns != 'Class']\n",
    "y= train.ix[:, train.columns == 'Class']\n",
    "fraud_count = len(train[train.Class == 1])\n",
    "fraud_indices = train[train.Class == 1].index\n",
    "normal_indices = train[train.Class == 0].index\n",
    "\n",
    "r_normal_indices = np.random.choice(normal_indices, fraud_count, replace = False) # random \n",
    "\n",
    "undersample_indices = np.concatenate([fraud_indices,r_normal_indices])\n",
    "undersample_train = train.iloc[undersample_indices,:]\n",
    "\n",
    "X_undersample = undersample_train.ix[:, undersample_train.columns != 'Class']\n",
    "y_undersample = undersample_train.ix[:, undersample_train.columns == 'Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "X_train_res, X_test_res, y_train_res, y_test_res= train_test_split(X_undersample,y_undersample,test_size = 0.3,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy score:0.942567567568\n",
      "[[144   5]\n",
      " [ 12 135]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.94       149\n",
      "          1       0.96      0.92      0.94       147\n",
      "\n",
      "avg / total       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train_res,y_train_res)\n",
    "testscoreLR=accuracy_score(y_test_res,lr.predict(X_test_res))\n",
    "print('logistic regression accuracy score:'+str(testscoreLR))\n",
    "print(confusion_matrix(y_test_res,lr.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,lr.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy score:0.929054054054\n",
      "[[144   5]\n",
      " [ 16 131]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93       149\n",
      "          1       0.96      0.89      0.93       147\n",
      "\n",
      "avg / total       0.93      0.93      0.93       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train_res,y_train_res)\n",
    "testscoreDT=accuracy_score(y_test_res,dt.predict(X_test_res))\n",
    "print('Decision Tree accuracy score:'+str(testscoreDT))\n",
    "print(confusion_matrix(y_test_res,dt.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,dt.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy score:0.915540540541\n",
      "[[135  14]\n",
      " [ 11 136]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.91      0.92       149\n",
      "          1       0.91      0.93      0.92       147\n",
      "\n",
      "avg / total       0.92      0.92      0.92       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(max_depth=8)\n",
    "dt.fit(X_train_res,y_train_res)\n",
    "testscoreDT=accuracy_score(y_test_res,dt.predict(X_test_res))\n",
    "print('Decision Tree accuracy score:'+str(testscoreDT))\n",
    "print(confusion_matrix(y_test_res,dt.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,dt.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy score:0.949324324324\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf= RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_res,y_train_res)\n",
    "testscoreRF=accuracy_score(y_test_res,rf.predict(X_test_res))\n",
    "print('Random Forest accuracy score:'+str(testscoreRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144   5]\n",
      " [ 10 137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95       149\n",
      "          1       0.96      0.93      0.95       147\n",
      "\n",
      "avg / total       0.95      0.95      0.95       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_res,rf.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,rf.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy score:0.915540540541\n",
      "[[145   4]\n",
      " [ 21 126]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.97      0.92       149\n",
      "          1       0.97      0.86      0.91       147\n",
      "\n",
      "avg / total       0.92      0.92      0.92       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gaussian navie bayers\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train_res,y_train_res)\n",
    "testscoreNB=accuracy_score(y_test_res,nb.predict(X_test_res))\n",
    "print('Decision Tree accuracy score:'+str(testscoreNB))\n",
    "print(confusion_matrix(y_test_res,nb.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,nb.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy score:0.945945945946\n",
      "[[146   3]\n",
      " [ 13 134]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95       149\n",
      "          1       0.98      0.91      0.94       147\n",
      "\n",
      "avg / total       0.95      0.95      0.95       296\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "kn=KNeighborsClassifier()\n",
    "kn.fit(X_train_res,y_train_res)\n",
    "testscoreKN=accuracy_score(y_test_res,kn.predict(X_test_res))\n",
    "print('Decision Tree accuracy score:'+str(testscoreKN))\n",
    "print(confusion_matrix(y_test_res,kn.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,kn.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy score:0.939189189189\n",
      "[[141   8]\n",
      " [ 10 137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94       149\n",
      "          1       0.94      0.93      0.94       147\n",
      "\n",
      "avg / total       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "gbm0.fit(X_train_res,y_train_res)\n",
    "testscoreGBM=accuracy_score(y_test_res,gbm0.predict(X_test_res))\n",
    "print('Decision Tree accuracy score:'+str(testscoreGBM))\n",
    "print(confusion_matrix(y_test_res,gbm0.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,gbm0.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy score:0.935810810811\n",
      "[[141   8]\n",
      " [ 11 136]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94       149\n",
      "          1       0.94      0.93      0.93       147\n",
      "\n",
      "avg / total       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "sv=svm.SVC()\n",
    "sv.fit(X_train_res,y_train_res)\n",
    "testscoreSV=accuracy_score(y_test_res,sv.predict(X_test_res))\n",
    "print('SVM accuracy score:'+str(testscoreSV))\n",
    "print(confusion_matrix(y_test_res,sv.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,sv.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy score:0.932432432432\n",
      "[[139  10]\n",
      " [ 10 137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93       149\n",
      "          1       0.93      0.93      0.93       147\n",
      "\n",
      "avg / total       0.93      0.93      0.93       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "mlp.fit(X_train_res,y_train_res)\n",
    "testscoreMLP=accuracy_score(y_test_res,mlp.predict(X_test_res))\n",
    "print('Neural Network accuracy score:'+str(testscoreMLP))\n",
    "print(confusion_matrix(y_test_res,mlp.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,mlp.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testing_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random Forest</td>\n",
       "      <td>0.949324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN Regression</td>\n",
       "      <td>0.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.942568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.939189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.935811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGS BOoster class.</td>\n",
       "      <td>0.932432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.915541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gaussian Naive Bays</td>\n",
       "      <td>0.915541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Testing_Score\n",
       "0        random Forest       0.949324\n",
       "3       KNN Regression       0.945946\n",
       "2  Logistic Regression       0.942568\n",
       "7                  SVM       0.939189\n",
       "6       Neural Network       0.935811\n",
       "5   XGS BOoster class.       0.932432\n",
       "1        Decision Tree       0.915541\n",
       "4  Gaussian Naive Bays       0.915541"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arrrange the model according tp there accuracy score\n",
    "models = pd.DataFrame({'Model' : [ 'random Forest', 'Decision Tree', 'Logistic Regression', 'KNN Regression','Gaussian Naive Bays','XGS BOoster class.','Neural Network','SVM'],'Testing_Score' : [ testscoreRF, testscoreDT, testscoreLR, testscoreKN, testscoreNB, testscoreMLP, testscoreSV, testscoreGBM ],})\n",
    "models.sort_values(by='Testing_Score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature ranking:\n",
      "feature no. 1: V14 (0.171337350124)\n",
      "feature no. 2: V10 (0.167106939627)\n",
      "feature no. 3: V4 (0.0895051291192)\n",
      "feature no. 4: V12 (0.0888708961729)\n",
      "feature no. 5: V11 (0.0847999297786)\n",
      "feature no. 6: V17 (0.0828941224532)\n",
      "feature no. 7: V3 (0.0521834228515)\n",
      "feature no. 8: V7 (0.0396923710329)\n",
      "feature no. 9: V16 (0.0221338412481)\n",
      "feature no. 10: V2 (0.0217953561732)\n",
      "feature no. 11: V9 (0.0173637066924)\n",
      "feature no. 12: V21 (0.0162393851427)\n",
      "feature no. 13: Amount_n (0.0146078230893)\n",
      "feature no. 14: V27 (0.013505028145)\n",
      "feature no. 15: V20 (0.0134798596873)\n",
      "feature no. 16: V19 (0.0110303327149)\n",
      "feature no. 17: V1 (0.009815618398)\n",
      "feature no. 18: V8 (0.00945236820064)\n",
      "feature no. 19: V28 (0.00889488281023)\n",
      "feature no. 20: V5 (0.00865787661611)\n",
      "feature no. 21: V15 (0.00856459528447)\n",
      "feature no. 22: V18 (0.00855876657369)\n",
      "feature no. 23: V26 (0.00702626199127)\n",
      "feature no. 24: V13 (0.00688814740073)\n",
      "feature no. 25: V23 (0.00671725184707)\n",
      "feature no. 26: V6 (0.00580539067621)\n",
      "feature no. 27: V25 (0.0052688801586)\n",
      "feature no. 28: V22 (0.00395031565847)\n",
      "feature no. 29: V24 (0.00385415033189)\n"
     ]
    }
   ],
   "source": [
    "#using Random Forest\n",
    "importances=rf.feature_importances_\n",
    "f=np.argsort(importances)[::-1]\n",
    "print ('feature ranking:')\n",
    "for i in range(X.shape[1]):\n",
    "     print (\"feature no. {}: {} ({})\".format(i+1,X.columns[f[i]],importances[f[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12ef318d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEqCAYAAADu0BDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4gESMVdRCRAKu4iIgH6/wIlzWL40H4GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ef31f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'feature':X_tr.columns,'importance':np.round(rf.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=True).set_index('feature')\n",
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature ranking:\n",
      "feature no. 1: V14 (0.768890875552)\n",
      "feature no. 2: V4 (0.0402297177364)\n",
      "feature no. 3: V10 (0.0315295947549)\n",
      "feature no. 4: V16 (0.0303586981305)\n",
      "feature no. 5: V19 (0.0294575535267)\n",
      "feature no. 6: V20 (0.0221281975952)\n",
      "feature no. 7: V8 (0.0132535729497)\n",
      "feature no. 8: V17 (0.0117497781951)\n",
      "feature no. 9: Amount_n (0.0117277414421)\n",
      "feature no. 10: V11 (0.00886096020067)\n",
      "feature no. 11: V22 (0.00886096020067)\n",
      "feature no. 12: V7 (0.00567630629084)\n",
      "feature no. 13: V21 (0.00545289858502)\n",
      "feature no. 14: V15 (0.00541503123374)\n",
      "feature no. 15: V12 (0.00362324040055)\n",
      "feature no. 16: V26 (0.00278487320592)\n",
      "feature no. 17: V28 (0.0)\n",
      "feature no. 18: V18 (0.0)\n",
      "feature no. 19: V13 (0.0)\n",
      "feature no. 20: V23 (0.0)\n",
      "feature no. 21: V24 (0.0)\n",
      "feature no. 22: V9 (0.0)\n",
      "feature no. 23: V25 (0.0)\n",
      "feature no. 24: V6 (0.0)\n",
      "feature no. 25: V5 (0.0)\n",
      "feature no. 26: V27 (0.0)\n",
      "feature no. 27: V3 (0.0)\n",
      "feature no. 28: V2 (0.0)\n",
      "feature no. 29: V1 (0.0)\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "importances=dt.feature_importances_\n",
    "f=np.argsort(importances)[::-1]\n",
    "print ('feature ranking:')\n",
    "for i in range(X.shape[1]):\n",
    "     print (\"feature no. {}: {} ({})\".format(i+1,X.columns[f[i]],importances[f[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10f7e4cd0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEsCAYAAADeoDiOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niATuvwHcYFLfIj0GLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1135499d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'feature':X_tr.columns,'importance':np.round(dt.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=True).set_index('feature')\n",
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x117ad8cd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAETCAYAAADUAmpRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZsD/B0vCr6AaECMgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117b3a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predprob_u = lr.predict_proba(X_test_res)[:, 1]  # default threshold 0.5\n",
    "\n",
    "plt.hist(y_predprob_u, bins=8)\n",
    "plt.xlabel('predicted probability of fraud')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Histogram of predicted probabilities') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-0f6c98349dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my1_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my1_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my2_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test_res' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test_res)\n",
    "y_prob = lr.predict_proba(X_test_res)\n",
    "y1_pred = dt.predict(X_test_res)\n",
    "y1_prob = dt.predict_proba(X_test_res)\n",
    "y2_prob = rf.predict_proba(X_test_res)\n",
    "y3_prob = nb.predict_proba(X_test_res)\n",
    "y4_prob = kn.predict_proba(X_test_res)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# area under the curve\n",
    "Falsepositive, truepositive,_ = (roc_curve(y_test,y_prob[:,1]))\n",
    "FalsepositiveDT, truepositiveDT,_ = (roc_curve(y_test,y1_prob[:,1]))\n",
    "FalsepositiveRF, truepositiveRF,_ = (roc_curve(y_test,y2_prob[:,1]))\n",
    "FalsepositiveNB, truepositiveNB,_ = (roc_curve(y_test,y3_prob[:,1]))\n",
    "FalsepositiveKN, truepositiveKN,_ = (roc_curve(y_test,y4_prob[:,1]))\n",
    "ROC_AUCLR = auc(Falsepositive, truepositive)\n",
    "ROC_AUCDT = auc(FalsepositiveDT, truepositiveDT)\n",
    "ROC_AUCRF = auc(FalsepositiveDT, truepositiveDT)\n",
    "ROC_AUCNB = auc(FalsepositiveNB, truepositiveNB)\n",
    "ROC_AUCKN = auc(FalsepositiveKN, truepositiveKN)\n",
    "#Plottig\n",
    "plt.plot(Falsepositive, truepositive, label='Logistic Regression (area = %0.2f)' % ROC_AUCLR, linewidth=4)\n",
    "plt.plot(FalsepositiveDT, truepositiveDT, label='Dicision Tree (area = %0.2f)' % ROC_AUCDT, linewidth=3)\n",
    "plt.plot(FalsepositiveRF, truepositiveRF, label='Random Forest (area = %0.2f)' % ROC_AUCRF, linewidth=1)\n",
    "plt.plot(FalsepositiveNB, truepositiveNB, label='Navie bayes(area = %0.2f)' % ROC_AUCNB, linewidth=2)\n",
    "plt.plot(FalsepositiveKN, truepositiveKN, label='KN(area = %0.2f)' % ROC_AUCNB, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC curve ', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...         V20       V21       V22  \\\n",
       "0  0.098698  0.363787  0.090794  ...    0.251412 -0.018307  0.277838   \n",
       "1  0.085102 -0.255425 -0.166974  ...   -0.069083 -0.225775 -0.638672   \n",
       "2  0.247676 -1.514654  0.207643  ...    0.524980  0.247998  0.771679   \n",
       "3  0.377436 -1.387024 -0.054952  ...   -0.208038 -0.108300  0.005274   \n",
       "4 -0.270533  0.817739  0.753074  ...    0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= df.drop(['Time','Amount'], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X= train.ix[:, train.columns != 'Class']\n",
    "y= train.ix[:, train.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdhankhad/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "fraud_count = len(train[train.Class == 1])\n",
    "fraud_indices = train[train.Class == 1].index\n",
    "normal_indices = train[train.Class == 0].index\n",
    "\n",
    "r_normal_indices = np.random.choice(normal_indices, fraud_count, replace = False) # random \n",
    "\n",
    "undersample_indices = np.concatenate([fraud_indices,r_normal_indices])\n",
    "undersample_train = train.iloc[undersample_indices,:]\n",
    "\n",
    "X_undersample = undersample_train.ix[:, undersample_train.columns != 'Class']\n",
    "y_undersample = undersample_train.ix[:, undersample_train.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('esampled Dataset has shape: ', (984, 28))\n",
      "('Total number of case of Real&duplicate ', 492)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Apply SMOTE's\n",
    "kind = 'regular'\n",
    "sm = SMOTE(kind='regular')\n",
    "X_res, y_res = sm.fit_sample(X,y)\n",
    "\n",
    "print(\"esampled Dataset has shape: \", X_res.shape)\n",
    "print(\"Total number of case of Real&duplicate \", np.sum(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split was successful.\n",
      "('Training set volume:', 213605)\n",
      "('Test set volume:', 71202)\n",
      "('Total number of transactions: ', 284807)\n",
      "\n",
      "('Number transactions train dataset: ', 738)\n",
      "('Number transactions test dataset: ', 246)\n",
      "('Total number of transactions: ', 984)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "print(\"Training and testing split was successful.\")\n",
    "print('Training set volume:', X_train.shape[0])\n",
    "print('Test set volume:', X_test.shape[0])\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
    "\n",
    "X_train_res, X_test_res, y_train_res, y_test_res= train_test_split(X_res, y_res)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", len(X_train_res))\n",
    "print(\"Number transactions test dataset: \", len(X_test_res))\n",
    "print(\"Total number of transactions: \", len(X_train_res)+len(X_test_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy score:0.930894308943\n",
      "[[122   6]\n",
      " [ 11 107]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.93       128\n",
      "          1       0.95      0.91      0.93       118\n",
      "\n",
      "avg / total       0.93      0.93      0.93       246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train_res,y_train_res)\n",
    "testscoreLR=accuracy_score(y_test_res,lr.predict(X_test_res))\n",
    "print('logistic regression accuracy score:'+str(testscoreLR))\n",
    "print(confusion_matrix(y_test_res,lr.predict(X_test_res)))\n",
    "print(classification_report(y_test_res,lr.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
